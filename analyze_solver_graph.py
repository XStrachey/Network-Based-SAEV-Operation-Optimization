#!/usr/bin/env python3
"""
åˆ†æ _08_build_solver_graph.py ç”Ÿæˆçš„å›¾è§„æ¨¡å’Œå¼§ç±»å‹ç»Ÿè®¡è„šæœ¬

è¯¥è„šæœ¬è¯»å– solver_graph ç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶ï¼Œè¾“å‡ºè¯¦ç»†çš„å›¾è§„æ¨¡å’Œå¼§ç±»å‹åˆ†ææŠ¥å‘Šã€‚

ä½¿ç”¨æ–¹æ³•:
    python analyze_solver_graph.py [--data-dir DIR] [--output-format FORMAT] [--save-report FILE]
    
å‚æ•°:
    --data-dir: æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: src/data/solver_graph)
    --output-format: è¾“å‡ºæ ¼å¼ (console/json/csv, é»˜è®¤: console)
    --save-report: ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (å¯é€‰)
"""

import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, Any


def load_solver_graph_data(data_dir: str = "src/data/solver_graph") -> Dict[str, Any]:
    """
    åŠ è½½æ±‚è§£å™¨å›¾æ•°æ®
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        
    Returns:
        åŒ…å«å…ƒæ•°æ®ã€èŠ‚ç‚¹å’Œå¼§æ•°æ®çš„å­—å…¸
    """
    data_path = Path(data_dir)
    
    # è¯»å–å…ƒæ•°æ®
    meta_path = data_path / "meta.json"
    if not meta_path.exists():
        raise FileNotFoundError(f"å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {meta_path}")
    
    with open(meta_path, 'r', encoding='utf-8') as f:
        meta = json.load(f)
    
    # è¯»å–èŠ‚ç‚¹æ•°æ®
    nodes_path = data_path / "nodes.parquet"
    if not nodes_path.exists():
        raise FileNotFoundError(f"èŠ‚ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {nodes_path}")
    
    nodes_df = pd.read_parquet(nodes_path)
    
    # è¯»å–å¼§æ•°æ®
    arcs_path = data_path / "arcs.parquet"
    if not arcs_path.exists():
        raise FileNotFoundError(f"å¼§æ–‡ä»¶ä¸å­˜åœ¨: {arcs_path}")
    
    arcs_df = pd.read_parquet(arcs_path)
    
    return {
        "meta": meta,
        "nodes": nodes_df,
        "arcs": arcs_df
    }


def analyze_graph_scale(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æå›¾è§„æ¨¡ä¿¡æ¯
    
    Args:
        data: åŒ…å«å…ƒæ•°æ®ã€èŠ‚ç‚¹å’Œå¼§æ•°æ®çš„å­—å…¸
        
    Returns:
        å›¾è§„æ¨¡åˆ†æç»“æœ
    """
    meta = data["meta"]
    nodes_df = data["nodes"]
    arcs_df = data["arcs"]
    
    # åŸºæœ¬è§„æ¨¡ä¿¡æ¯
    scale_info = {
        "basic": {
            "nodes_total": len(nodes_df),
            "arcs_total": len(arcs_df),
            "time_window": {
                "t0": meta.get("t0"),
                "H": meta.get("H"),
                "t_hi": meta.get("t_hi")
            },
            "supply_total": meta.get("sup_total", 0.0)
        }
    }
    
    # èŠ‚ç‚¹åˆ†æ
    nodes_with_supply = (nodes_df["supply"] > 0).sum()
    nodes_with_negative_supply = (nodes_df["supply"] < 0).sum()
    
    scale_info["nodes"] = {
        "total": len(nodes_df),
        "with_positive_supply": nodes_with_supply,
        "with_negative_supply": nodes_with_negative_supply,
        "with_zero_supply": len(nodes_df) - nodes_with_supply - nodes_with_negative_supply,
        "time_range": {
            "min": float(nodes_df["t"].min()) if "t" in nodes_df.columns else None,
            "max": float(nodes_df["t"].max()) if "t" in nodes_df.columns else None
        },
        "soc_range": {
            "min": float(nodes_df["soc"].min()) if "soc" in nodes_df.columns else None,
            "max": float(nodes_df["soc"].max()) if "soc" in nodes_df.columns else None
        },
        "zones_count": int(nodes_df["zone"].nunique()) if "zone" in nodes_df.columns else None
    }
    
    # å¼§åˆ†æ
    scale_info["arcs"] = {
        "total": len(arcs_df),
        "unique_arc_types": int(arcs_df["arc_type"].nunique()) if "arc_type" in arcs_df.columns else 0,
        "cost_range": {
            "min": float(arcs_df["cost"].min()) if "cost" in arcs_df.columns else None,
            "max": float(arcs_df["cost"].max()) if "cost" in arcs_df.columns else None,
            "mean": float(arcs_df["cost"].mean()) if "cost" in arcs_df.columns else None
        },
        "capacity_range": {
            "min": float(arcs_df["capacity"].min()) if "capacity" in arcs_df.columns else None,
            "max": float(arcs_df["capacity"].max()) if "capacity" in arcs_df.columns else None,
            "mean": float(arcs_df["capacity"].mean()) if "capacity" in arcs_df.columns else None
        }
    }
    
    return scale_info


def analyze_arc_types(data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æå¼§ç±»å‹åˆ†å¸ƒ
    
    Args:
        data: åŒ…å«å…ƒæ•°æ®ã€èŠ‚ç‚¹å’Œå¼§æ•°æ®çš„å­—å…¸
        
    Returns:
        å¼§ç±»å‹åˆ†æç»“æœ
    """
    arcs_df = data["arcs"]
    
    if "arc_type" not in arcs_df.columns:
        return {"error": "å¼§æ•°æ®ä¸­ç¼ºå°‘ arc_type åˆ—"}
    
    # å¼§ç±»å‹ç»Ÿè®¡
    arc_type_counts = arcs_df["arc_type"].value_counts().sort_index()
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    total_arcs = len(arcs_df)
    arc_type_percentages = (arc_type_counts / total_arcs * 100).round(2)
    
    # å¼§ç±»å‹è¯¦ç»†åˆ†æ
    arc_type_details = {}
    for arc_type in arc_type_counts.index:
        subset = arcs_df[arcs_df["arc_type"] == arc_type]
        
        details = {
            "count": int(arc_type_counts[arc_type]),
            "percentage": float(arc_type_percentages[arc_type]),
            "cost_stats": {
                "min": float(subset["cost"].min()) if "cost" in subset.columns else None,
                "max": float(subset["cost"].max()) if "cost" in subset.columns else None,
                "mean": float(subset["cost"].mean()) if "cost" in subset.columns else None
            },
            "capacity_stats": {
                "min": float(subset["capacity"].min()) if "capacity" in subset.columns else None,
                "max": float(subset["capacity"].max()) if "capacity" in subset.columns else None,
                "mean": float(subset["capacity"].mean()) if "capacity" in subset.columns else None
            }
        }
        
        # å¦‚æœæœ‰æ—¶é—´ä¿¡æ¯ï¼Œåˆ†ææ—¶é—´åˆ†å¸ƒ
        if "t" in subset.columns:
            details["time_stats"] = {
                "min": float(subset["t"].min()),
                "max": float(subset["t"].max()),
                "unique_times": int(subset["t"].nunique())
            }
        
        arc_type_details[arc_type] = details
    
    return {
        "summary": {
            "total_arc_types": len(arc_type_counts),
            "dominant_type": arc_type_counts.index[arc_type_counts.argmax()],
            "dominant_count": int(arc_type_counts.max()),
            "dominant_percentage": float(arc_type_percentages.max())
        },
        "counts": arc_type_counts.to_dict(),
        "percentages": arc_type_percentages.to_dict(),
        "details": arc_type_details
    }


def print_analysis_report(scale_info: Dict[str, Any], arc_types_info: Dict[str, Any], meta: Dict[str, Any]):
    """
    æ‰“å°åˆ†ææŠ¥å‘Š
    
    Args:
        scale_info: å›¾è§„æ¨¡åˆ†æç»“æœ
        arc_types_info: å¼§ç±»å‹åˆ†æç»“æœ
        meta: å…ƒæ•°æ®
    """
    print("=" * 80)
    print("æ±‚è§£å™¨å›¾è§„æ¨¡ä¸å¼§ç±»å‹åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    
    # åŸºæœ¬ä¿¡æ¯
    print("\nğŸ“Š åŸºæœ¬ä¿¡æ¯")
    print("-" * 40)
    basic = scale_info["basic"]
    print(f"æ—¶é—´çª—å£: t0={basic['time_window']['t0']}, H={basic['time_window']['H']}, t_hi={basic['time_window']['t_hi']}")
    print(f"èŠ‚ç‚¹æ€»æ•°: {basic['nodes_total']:,}")
    print(f"å¼§æ€»æ•°: {basic['arcs_total']:,}")
    print(f"ä¾›ç»™æ€»é‡: {basic['supply_total']}")
    
    # èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯
    print("\nğŸ”¢ èŠ‚ç‚¹è¯¦ç»†ä¿¡æ¯")
    print("-" * 40)
    nodes_info = scale_info["nodes"]
    print(f"æ€»èŠ‚ç‚¹æ•°: {nodes_info['total']:,}")
    print(f"æ­£ä¾›ç»™èŠ‚ç‚¹æ•°: {nodes_info['with_positive_supply']:,}")
    print(f"è´Ÿä¾›ç»™èŠ‚ç‚¹æ•°: {nodes_info['with_negative_supply']:,}")
    print(f"é›¶ä¾›ç»™èŠ‚ç‚¹æ•°: {nodes_info['with_zero_supply']:,}")
    
    if nodes_info["time_range"]["min"] is not None:
        print(f"æ—¶é—´èŒƒå›´: {nodes_info['time_range']['min']:.0f} - {nodes_info['time_range']['max']:.0f}")
    if nodes_info["soc_range"]["min"] is not None:
        print(f"SOCèŒƒå›´: {nodes_info['soc_range']['min']:.0f} - {nodes_info['soc_range']['max']:.0f}")
    if nodes_info["zones_count"] is not None:
        print(f"åŒºåŸŸæ•°é‡: {nodes_info['zones_count']}")
    
    # å¼§è¯¦ç»†ä¿¡æ¯
    print("\nğŸ”— å¼§è¯¦ç»†ä¿¡æ¯")
    print("-" * 40)
    arcs_info = scale_info["arcs"]
    print(f"æ€»å¼§æ•°: {arcs_info['total']:,}")
    print(f"å¼§ç±»å‹æ•°: {arcs_info['unique_arc_types']}")
    
    if arcs_info["cost_range"]["min"] is not None:
        print(f"æˆæœ¬èŒƒå›´: {arcs_info['cost_range']['min']:.4f} - {arcs_info['cost_range']['max']:.4f}")
        print(f"å¹³å‡æˆæœ¬: {arcs_info['cost_range']['mean']:.4f}")
    
    if arcs_info["capacity_range"]["min"] is not None:
        print(f"å®¹é‡èŒƒå›´: {arcs_info['capacity_range']['min']:.0f} - {arcs_info['capacity_range']['max']:.0f}")
        print(f"å¹³å‡å®¹é‡: {arcs_info['capacity_range']['mean']:.0f}")
    
    # å¼§ç±»å‹åˆ†å¸ƒ
    print("\nğŸ“ˆ å¼§ç±»å‹åˆ†å¸ƒ")
    print("-" * 40)
    
    if "error" in arc_types_info:
        print(f"é”™è¯¯: {arc_types_info['error']}")
        return
    
    summary = arc_types_info["summary"]
    print(f"å¼§ç±»å‹æ€»æ•°: {summary['total_arc_types']}")
    print(f"ä¸»å¯¼ç±»å‹: {summary['dominant_type']} ({summary['dominant_count']:,} æ¡, {summary['dominant_percentage']:.1f}%)")
    
    print("\nè¯¦ç»†åˆ†å¸ƒ:")
    print(f"{'å¼§ç±»å‹':<15} {'æ•°é‡':<12} {'ç™¾åˆ†æ¯”':<8} {'å¹³å‡æˆæœ¬':<12} {'å¹³å‡å®¹é‡':<12}")
    print("-" * 65)
    
    for arc_type, details in arc_types_info["details"].items():
        cost_mean = details["cost_stats"]["mean"]
        capacity_mean = details["capacity_stats"]["mean"]
        
        cost_str = f"{cost_mean:.4f}" if cost_mean is not None else "N/A"
        capacity_str = f"{capacity_mean:.0f}" if capacity_mean is not None else "N/A"
        
        print(f"{arc_type:<15} {details['count']:<12,} {details['percentage']:<8.1f}% {cost_str:<12} {capacity_str:<12}")
    
    # ç‰¹æ®ŠèŠ‚ç‚¹ä¿¡æ¯
    print("\nğŸ¯ ç‰¹æ®ŠèŠ‚ç‚¹ä¿¡æ¯")
    print("-" * 40)
    if meta.get("sink_node_id") is not None:
        print(f"è¶…çº§æ±‡ç‚¹èŠ‚ç‚¹ID: {meta['sink_node_id']}")
    
    print("\n" + "=" * 80)


def convert_numpy_types(obj):
    """é€’å½’è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, pd.Timestamp):
        return obj.isoformat()
    else:
        return obj


def save_json_report(scale_info: Dict[str, Any], arc_types_info: Dict[str, Any], 
                     meta: Dict[str, Any], output_file: str):
    """ä¿å­˜JSONæ ¼å¼æŠ¥å‘Š"""
    report = {
        "metadata": convert_numpy_types(meta),
        "scale_analysis": convert_numpy_types(scale_info),
        "arc_types_analysis": convert_numpy_types(arc_types_info),
        "generated_at": pd.Timestamp.now().isoformat()
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def save_csv_report(scale_info: Dict[str, Any], arc_types_info: Dict[str, Any], 
                    meta: Dict[str, Any], output_file: str):
    """ä¿å­˜CSVæ ¼å¼æŠ¥å‘Š"""
    # åˆ›å»ºå¼§ç±»å‹ç»Ÿè®¡è¡¨
    arc_types_data = []
    for arc_type, details in arc_types_info["details"].items():
        arc_types_data.append({
            "arc_type": arc_type,
            "count": details["count"],
            "percentage": details["percentage"],
            "avg_cost": details["cost_stats"]["mean"],
            "avg_capacity": details["capacity_stats"]["mean"]
        })
    
    arc_types_df = pd.DataFrame(arc_types_data)
    
    # åˆ›å»ºåŸºæœ¬ä¿¡æ¯è¡¨
    basic_info = pd.DataFrame([{
        "metric": "total_nodes",
        "value": scale_info["basic"]["nodes_total"]
    }, {
        "metric": "total_arcs", 
        "value": scale_info["basic"]["arcs_total"]
    }, {
        "metric": "total_supply",
        "value": scale_info["basic"]["supply_total"]
    }, {
        "metric": "time_window_t0",
        "value": scale_info["basic"]["time_window"]["t0"]
    }, {
        "metric": "time_window_H",
        "value": scale_info["basic"]["time_window"]["H"]
    }, {
        "metric": "time_window_t_hi",
        "value": scale_info["basic"]["time_window"]["t_hi"]
    }])
    
    # ä¿å­˜åˆ°CSVæ–‡ä»¶
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        arc_types_df.to_excel(writer, sheet_name='Arc_Types', index=False)
        basic_info.to_excel(writer, sheet_name='Basic_Info', index=False)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="åˆ†æ _08_build_solver_graph.py ç”Ÿæˆçš„å›¾è§„æ¨¡å’Œå¼§ç±»å‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python analyze_solver_graph.py
  python analyze_solver_graph.py --data-dir custom/path
  python analyze_solver_graph.py --output-format json --save-report report.json
  python analyze_solver_graph.py --output-format csv --save-report report.xlsx
        """
    )
    
    parser.add_argument(
        "--data-dir",
        default="src/data/solver_graph",
        help="æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: src/data/solver_graph)"
    )
    
    parser.add_argument(
        "--output-format",
        choices=["console", "json", "csv"],
        default="console",
        help="è¾“å‡ºæ ¼å¼ (é»˜è®¤: console)"
    )
    
    parser.add_argument(
        "--save-report",
        help="ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ (JSONæˆ–Excelæ ¼å¼)"
    )
    
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºé”™è¯¯ä¿¡æ¯"
    )
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    try:
        # åŠ è½½æ•°æ®
        if not args.quiet:
            print("æ­£åœ¨åŠ è½½æ±‚è§£å™¨å›¾æ•°æ®...")
        data = load_solver_graph_data(args.data_dir)
        
        # åˆ†æå›¾è§„æ¨¡
        if not args.quiet:
            print("æ­£åœ¨åˆ†æå›¾è§„æ¨¡...")
        scale_info = analyze_graph_scale(data)
        
        # åˆ†æå¼§ç±»å‹
        if not args.quiet:
            print("æ­£åœ¨åˆ†æå¼§ç±»å‹...")
        arc_types_info = analyze_arc_types(data)
        
        # æ ¹æ®è¾“å‡ºæ ¼å¼å¤„ç†ç»“æœ
        if args.output_format == "console":
            print_analysis_report(scale_info, arc_types_info, data["meta"])
        elif args.output_format == "json":
            if args.save_report:
                save_json_report(scale_info, arc_types_info, data["meta"], args.save_report)
            else:
                report = {
                    "metadata": data["meta"],
                    "scale_analysis": scale_info,
                    "arc_types_analysis": arc_types_info
                }
                print(json.dumps(report, indent=2, ensure_ascii=False))
        elif args.output_format == "csv":
            if args.save_report:
                save_csv_report(scale_info, arc_types_info, data["meta"], args.save_report)
            else:
                print("CSVæ ¼å¼éœ€è¦æŒ‡å®šè¾“å‡ºæ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ --save-report å‚æ•°")
                return 1
        
        # ä¿å­˜æŠ¥å‘Šï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if args.save_report and args.output_format == "console":
            # é»˜è®¤ä¿å­˜JSONæ ¼å¼
            save_json_report(scale_info, arc_types_info, data["meta"], args.save_report)
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())
